{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import json\n",
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'__v': 0,\n",
      "  '_chunks': [{'_id': '56aba31c9b1c17c8c853b420',\n",
      "               'ad_id': 4227,\n",
      "               'chunk_id': 88795,\n",
      "               'content': 'Key reponsibilities and tasks'},\n",
      "              {'_id': '56aba31c9b1c17c8c853b421',\n",
      "               'ad_id': 4227,\n",
      "               'chunk_id': 88796,\n",
      "               'content': 'Development of profitable customer solutions, with '\n",
      "                          'a holistic view on the customer needs and services '\n",
      "                          'delivered by Nets'},\n",
      "              {'_id': '56aba31c9b1c17c8c853b422',\n",
      "               'ad_id': 4227,\n",
      "               'chunk_id': 88797,\n",
      "               'content': 'Participate in customer pre-study projects and RFP '\n",
      "                          'processes'}],\n",
      "  '_id': '56af985e9b1c17c8c854751f',\n",
      "  'content': 'requirements',\n",
      "  'updated': '2016-02-01T17:39:42.543Z'}]\n"
     ]
    }
   ],
   "source": [
    "url = (\"http://localhost:8082/api/populatedTags\");\n",
    "response = requests.get(url)\n",
    "data = json.loads(response.text)\n",
    "pprint(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TagCorpus(object):\n",
    "    def __iter__(self):\n",
    "        for tag in data:\n",
    "            words = \"\"\n",
    "            for chunk in tag['_chunks']:\n",
    "                words += \" \" + str(chunk['content']);\n",
    "            yield words.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.TagCorpus object at 0x1093931d0>\n"
     ]
    }
   ],
   "source": [
    "corpus = TagCorpus() # doesn't load the corpus into memory!\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Key', 'reponsibilities', 'and', 'tasks', 'Development', 'of', 'profitable', 'customer', 'solutions,', 'with', 'a', 'holistic', 'view', 'on', 'the', 'customer', 'needs', 'and', 'services', 'delivered', 'by', 'Nets', 'Participate', 'in', 'customer', 'pre-study', 'projects', 'and', 'RFP', 'processes']\n"
     ]
    }
   ],
   "source": [
    "for vector in corpus: # load one vector into memory at a time\n",
    "    print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(21 unique tokens: ['Key', 'reponsibilities', 'services', 'customer', 'RFP']...)\n"
     ]
    }
   ],
   "source": [
    "stoplist = set('for a of the and to in'.split())\n",
    "# collect statistics about all tokens\n",
    "dictionary = corpora.Dictionary(vector for vector in corpus)\n",
    "# remove stop words and words that appear only once\n",
    "stop_ids = [dictionary.token2id[stopword] for stopword in stoplist\n",
    "            if stopword in dictionary.token2id]\n",
    "once_ids = [tokenid for tokenid, docfreq in dictionary.dfs.items() if docfreq == 1]\n",
    "\n",
    "# dictionary.filter_tokens(stop_ids + once_ids) # remove stop words and words that appear only once\n",
    "dictionary.filter_tokens(stop_ids)\n",
    "\n",
    "dictionary.compactify() # remove gaps in id sequence after words that were removed\n",
    "print(dictionary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

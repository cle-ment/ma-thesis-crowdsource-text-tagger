{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b style=\"float: right\">Clemens Westrup</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This document describes and implements a prototype to classify job ads using the TF.IDF technique to create a vector space of words. In this space the distance between classes can be measured and new documents are classified by mapping their bag-of-words representation into a vector in the same space with TF.IDF and computing the cosine or jaccard distance to the nearest classes.\n",
    "\n",
    "This ipython/jupyter notebook can be downloaded and used interactively and contains all code ready to run. As prerequisites python3 must be installed with the packages listed below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "import scipy\n",
    "import datetime\n",
    "import sklearn.metrics\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset: English Job Ads from Oikotie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to retrieve labels for the data I downloaded around 6000 english job ads from the Oikotie job platform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These were then split into sections to be classified. The splitting criteria were the character sequences: \"&lt;br&gt;\", \"&lt;/p&gt;\" and the unix linebreak character. The code for preprocessing the ads can be found here <https://github.com/cle-ment/thesis-tagger/blob/master/pre-processing.ipynb>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labelling the Data with a Little Help of My Friends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then build a tool to ask others to help my manually tag job ads. This tool consists of a Node.js server with using MongoDB for persistence and a HTML5/JS interface. The code and description can be found on GitHub: <https://github.com/cle-ment/thesis-tagger>, and the interface is online here: <http://thesis.cwestrup.de/jobad-tagger/>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The server offers a simple REST API do send and retrieve the job ad data and tags which is documented here: <http://thesis.cwestrup.de/jobad-tagger/apidoc/> and can be accessed via <http://thesis.cwestrup.de/api>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the point of writing there were a total of 60 submitted labelled job ads resulting in 304 tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the resulting labelled data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting labelled data can be downloaded using the API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def downloadTaggedChunks():\n",
    "    print(\"Retrieved 0 tagged chunks. Working...\", end=\"\\r\")\n",
    "\n",
    "    url = (\"http://cwestrup.de:8082/api/tags/populated\");\n",
    "    file = \"data/tagged_chunks\"+str(datetime.datetime.now())+\".json\"\n",
    "    last_batch_received = False\n",
    "    size = 100\n",
    "    page = 1\n",
    "\n",
    "    with open(file, 'w', encoding='utf8') as json_file:\n",
    "        json_file.write(\"[\")\n",
    "\n",
    "    while not last_batch_received:\n",
    "        params = {'size': size, 'page': page}\n",
    "        response = requests.get(url, params=params)\n",
    "        if response.status_code != 200: \n",
    "            print(\"Request not successful\")\n",
    "            break\n",
    "        # count items\n",
    "        json = response.json()\n",
    "        if (len(json) < size): last_batch_received = True\n",
    "        data = response.text.lstrip(\"[\").rstrip(\"]\")\n",
    "        with open(file, 'a', encoding='utf8') as json_file:\n",
    "            json_file.write(data)\n",
    "            if not last_batch_received:\n",
    "                json_file.write(', ')\n",
    "                print(\"Retrieved \" + str(page*size) + \" tagged chunks. Working...\", end=\"\\r\")\n",
    "                page += 1\n",
    "            else:\n",
    "                json_file.write(\"]\")\n",
    "                print(\"Retrieved \" + str((page-1)*size + len(json)) + \" tagged chunks. Done!\")\n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 316 tagged chunks. Done!\n"
     ]
    }
   ],
   "source": [
    "file = downloadTaggedChunks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the data we just downloaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"./\"+file) as data_file:\n",
    "    data = json.load(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify the number of chunks we downloaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "316"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at all the tags the participants used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "employer's information, language requirement, job id, website of employer, start of list, functions, travel requirements, start of skills list, auxiliary target group, benefits, introduction to organization, expected values, #manpower #temps #recruiting, application process, company , help, employer, contact details, application procedure, website contact info, checklist for requirements , 90% bullshit, recruiter, attitude, heading, empy, competence, responsibilities heading, skills heading, practical information, detailed info, working hours, skill requirement, skill exception, contacts, basic info, tech, innovation, level, role, application, languages, company culture, personal, general job description, detailed task, introductory phrase , general task, general task , candidate's requirements, general company info, task, section title, requirement, #bayer #bayer2013 #bayerr&d, #clinicalcontractcoordinator #bayerrecruiting , #kemira #finlandjob, #seniormanager #mgmt, tracking, time, programming skills, number, time commitment, company's needs, company's treats, search, job, location of job, time frame, treats, website, job related field, prerequisite studies, communication skills, communication, duration, more about position, job form, contact agent, announcement, job title , job classification , employer listing, encouragement, invitation , signature , announcement , attachments, dates, extent, unit, unit description, contact ddetails, job role, contact person, job introduction , application submission, experience requirement, company description , further info, ask, travel, personal skill, team title, empty, header, offer, marketing the job, non-discrimination, organisatio, langugage skills, deadline for application, performance, employer info, job benefits, bonus skills, job inquiry email, quality assurance, asw, software development, mobile development, analysis, testing, manollo, agile, istqb, tools, a360, project management, bugtracking tool, programming languages, soft skills, company facts, company vision, about job agent , about job agent, time period, goal, further information, employer listing , duties, traveling requirements, employee qualities , requirements , contact , invitation, deadline , service offering, application details, work experience, wished skills, employer description, qualifications, company, position type, about, experience, opportunities, job agent, contact, introduction, language requirements, contact person title, phone number, opening date, closing date, reaktor, clojure, software, java, scala, company introduction, job duration , practicalities, location , checklist for job description , job description , requirement , application practicalities , contact practicalities , contact person name , contact person title , contact person mail , overview, degree, previous experience, social skills, intro to company, position description, actual requirements, signal for interview question, job specific tag, bullshit, general tasks, formal requirements, applicant's personality, applicant's abilities, additional job opportunities, incentive, language, desired skill, transition, additional opportunity, keywords, requirements; repeated, contact info, salary, job location, useless info, job title, job responsibilities, job requirements, job requirements; characteristics, nothing, dealine, work summary, domain skill, communication skill, location, selling the job, details, who, what, where, task titles, position level, responsibilities, habits, important requirements, application instructions, responsibilites, applying, job specific requirements, common requirements, application deadline, company presentation, about the role, how to apply?, language skill, subheader, more information, company description, background, personality, offering, organization, job description, job type, we offer, department, conditions, deadline, design, company policy, equal opportunity, work title, character, ccompany details, company details, product line, detailed information on productline, organisational position, additional information, organisation description, job desription, location data, job offer, main tasks, position, knwoledge, education, application information, contact information, tasks, summary, technical skills, job position, job number, job contact information, employment type, responsibility, marketing of the company, knowledge, compensation, crap, info, description, company info, skill, apply info, futrher information, sijainti, lisätieto, kuvaus, paikka, information, oppoturnity, more info, place, title, unit of the company, company name, industry, high level summary, requirements title, language skills, need, unit of company, field of operations, company unit descriptions, what you get, capability, expectations, how to apply, skills, requirements, "
     ]
    }
   ],
   "source": [
    "for document in data:\n",
    "    sys.stdout.write(document['content'] + \", \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Labelled Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first convert the data to a dictionary in the correct format for further processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_dict = {}\n",
    "for document in data:\n",
    "    for chunk in document['chunks']:\n",
    "        try:\n",
    "            data_dict[chunk['_chunk']['content']].add(document['content'])\n",
    "        except KeyError:\n",
    "            data_dict[chunk['_chunk']['content']] = set([document['content']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we'll define a list with stopwords taken from <http://www.ranks.nl/stopwords>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stopwords = (\"a about above after again against all am an and any are aren't as at be \"\n",
    "             + \"because been before being below between both but by can't cannot could \"\n",
    "             + \"couldn't did didn't do does doesn't doing don't down during each few for \"\n",
    "             + \"from further had hadn't has hasn't have haven't having he he'd he'll he's \"\n",
    "             + \"her here here's hers herself him himself his howhow's i i'd i'll i'm i've \"\n",
    "             + \"if in into is isn't it it's its itself let's me more most mustn't my myself \"\n",
    "             + \"no nor not of off on once only or other ought our ours ourselves out over \"\n",
    "             + \"own same shan't she she'd she'll she's should shouldn't so some such than \"\n",
    "             + \"that that's the their theirs them themselves then there there's these they \"\n",
    "             + \"they'd they'll they're they've this those through to too under until up very \"\n",
    "             + \"was wasn't we we'd we'll we're we've were weren't what what's when when's \"\n",
    "             + \"where where's which while who who's whom why why's with won't would wouldn't \"\n",
    "             + \"you you'd you'll you're you've your yours yourself yourselves - . :\").split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we need a function to \"lemmatize\" or process each word (very simple so far):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lemmatize(word):\n",
    "    # TODO: do a bit more sophisticated processing here\n",
    "    return word.lower().strip().strip(\".\").strip(\",\").strip(\":\").strip(\"-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Vocabulary for Words and Tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll build a vocabulary for all tags and words in the data and assign each of them to a unique id (the position in the array). At the same time the words are \"lemmatized\" or processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def buildVocab(data_dict):\n",
    "    vocab_words = [] # array to store all words (and their indices)\n",
    "    vocab_tags  = [] # array to store all tags (and their indices)\n",
    "\n",
    "    for chunk, tags in data_dict.items():        \n",
    "        if re.match(r'^\\s*$', chunk): continue # skip empty chunks\n",
    "        words = [lemmatize(word) for word in chunk.split() if not lemmatize(word) in stopwords]\n",
    "        for word in words:\n",
    "            # add word to vocabWords\n",
    "            try:\n",
    "                word_id = vocab_words.index(word)\n",
    "            except ValueError:\n",
    "                vocab_words.append(word)\n",
    "                word_id = len(vocab_words)\n",
    "\n",
    "        for tag in tags:\n",
    "            # append tag to tag vocab\n",
    "            try:\n",
    "                vocab_tags.index(lemmatize(tag))\n",
    "            except ValueError:\n",
    "                vocab_tags.append(lemmatize(tag))    \n",
    "                \n",
    "    return vocab_words, vocab_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab_words, vocab_tags = buildVocab(data_dict);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3165, 298)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_words), len(vocab_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need a couple of functions to convert the id's and strings for words and tags into one another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word2id(word, vocab_words):\n",
    "    return vocab_words.index(word)\n",
    "\n",
    "def id2word(id, vocab_words):\n",
    "    return vocab_words[id]\n",
    "\n",
    "def tag2id(tag, vocab_tags):\n",
    "    return vocab_tags.index(tag)\n",
    "\n",
    "def id2tag(id, vocab_tags):\n",
    "    return vocab_tags[id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing a Wordcount Matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll go through the dataset and create a sparse matrix of the form $words \\times tags$ with the frequency for each word in with each tag. In information retrieval terms the words here are terms and each tag represents a document (or rather the words assigned to it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def buildWordcountMatrix(data_dict, vocab_words, vocab_tags):\n",
    "    wordcounts = scipy.sparse.lil_matrix((len(vocab_words), len(vocab_tags)), dtype=int)\n",
    "    for chunk, tags in data_dict.items():\n",
    "        words = [word for word in chunk.split() if not lemmatize(word) in stopwords]\n",
    "        for word in words:\n",
    "            for tag in tags:   \n",
    "                tag_id = tag2id(lemmatize(tag), vocab_tags)               \n",
    "                word_id = word2id(lemmatize(word), vocab_words)\n",
    "                wordcounts[word_id, tag_id] += 1\n",
    "    \n",
    "    return wordcounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordcounts = buildWordcountMatrix(data_dict, vocab_words, vocab_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the Tf.Idf Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll convert the wordcount data into a TF.IDF matrix as in [1, section 1.2.1] with  \n",
    "\n",
    "$${TF}_{ij} = \\frac{f_{ij}}{{max}_k f_{kj}}$$\n",
    "\n",
    "$${IDF}_i = {log}_2(N/n_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def computeTFIDF(wordcounts):\n",
    "    max_term_freqs = np.amax(wordcounts.A, axis=0)\n",
    "    total_tags = wordcounts.shape[1]\n",
    "    total_words = wordcounts.shape[0]\n",
    "\n",
    "    # compute TF\n",
    "    # term/doc frequency / maximum frequency of all terms in that document\n",
    "    tf = wordcounts.tocsr().multiply(scipy.sparse.csr_matrix(1 / max_term_freqs))\n",
    "\n",
    "    # compute IDF\n",
    "    # total documents / num of documents with term i\n",
    "    idf = scipy.sparse.csr_matrix(np.log2(total_tags / wordcounts.tocsr().getnnz(axis=1)))\n",
    "\n",
    "    # compute TF.IDF\n",
    "    tfidf_matrix = tf.transpose().multiply(idf).transpose()\n",
    "    \n",
    "    return tfidf_matrix;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf_matrix = computeTFIDF(wordcounts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use this matrix to find the most similar words for each tag and vice versa and to find the most similar tags by computing the distance between them. Both of these side experiments can be found in the appendix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying New Texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To classify new texts we'll compute the TF.IDF vector for the new chunk and return the most similar tag vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def classifyChunk(text, tfidf_matrix, vocab_tags, vocab_words, \n",
    "                  distance_threshold=0.9, return_max=5):    \n",
    "    total_tags = len(vocab_tags)\n",
    "    total_words = len(vocab_words)\n",
    "    # collect frequencies of known words\n",
    "    occurrences = np.zeros((total_words,1))\n",
    "    words = text.split()      \n",
    "    words_found = False\n",
    "    for word in [lemmatize(word) for word in words]:\n",
    "        try:\n",
    "            word_id = word2id(word, vocab_words)\n",
    "            occurrences[word_id] += 1\n",
    "            words_found = True\n",
    "        except ValueError:\n",
    "            continue\n",
    "    # if no words found to classify return empty result\n",
    "    if not words_found: return []\n",
    "    # calculate TF.IDF\n",
    "    max_term_freq = occurrences.max()\n",
    "    num_of_docs_with_term = tfidf_matrix.getnnz(axis=1)\n",
    "    tf = occurrences / max_term_freq\n",
    "    idf = scipy.log2(total_tags / num_of_docs_with_term)\n",
    "    tfidf_vec = tf.reshape(-1) * idf\n",
    "    # compute distances to all tag vectors\n",
    "    distances = sklearn.metrics.pairwise_distances(tfidf_matrix.transpose(), \n",
    "                                                   tfidf_vec.reshape(1, -1), metric=\"cosine\")\n",
    "    # \n",
    "    distance_tuples = [(id2tag(id, vocab_tags), item[0]) \n",
    "                       for id, item in enumerate(distances) if item[0] < distance_threshold]\n",
    "    return sorted(distance_tuples, key=lambda x:x[1])[0:return_max]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = (\"Do you want to join our team? Please leave your application and CV no later \"\n",
    "        + \"than 20.9.2015: http://www.fennovoima.fi/en/jobs/open-positions. For further \"\n",
    "        + \"information, please contact Psycon consultant Rauna Kautto tel. 020 7101 242 \"\n",
    "        + \"on Wednesday 16.9. at 9-10 or on Friday 18.9. at 14-15. More information about \" \n",
    "        + \"Fennovoima you will find on our website: www.fennovoima.fi.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('application submission', 4.4408920985006262e-16),\n",
       " ('application instructions', 0.29764818254826508),\n",
       " ('contact information', 0.56496033241087573),\n",
       " ('contact', 0.60759132544406558),\n",
       " ('further info', 0.79354848870005745)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifyChunk(text, tfidf_matrix, vocab_tags, vocab_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also need a baseline for comparision. This will be a function that randomly guesses $n$ labels out of the available labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def classifyChunkRandomGuess(vocab_tags, return_max=5):\n",
    "    return np.random.choice(vocab_tags, return_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['bonus skills', 'signature', 'detailed information on productline',\n",
       "       'company culture', 'unit of the company'], \n",
       "      dtype='<U45')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifyChunkRandomGuess(vocab_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally to analyze the results let's define a function for cross-validating with the dataset. We'll predict max $n$ (3 by default) labels with the algorithm and interpret the result as a correct classification if there is an intersection between the predicted labels and the true labels. \n",
    "Also the crossvalidation can run the random guesser as a baseline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def crossvalidate(data_dict, tfidf_matrix, vocab_tags, vocab_words, folds=5, \n",
    "                  return_max=3, baseline=False, verbose=False):\n",
    "    correct_total = 0\n",
    "    test_size_total = 0\n",
    "\n",
    "    for i in range(folds):\n",
    "        # training set size 80%\n",
    "        train_set_size = math.floor(len(data_dict)*0.7)\n",
    "        test_set_size = len(data_dict) - train_set_size\n",
    "\n",
    "        correct = 0\n",
    "\n",
    "        # shuffle data and select subsets\n",
    "        keys = list(data_dict.keys()) \n",
    "        random.shuffle(keys)\n",
    "        train_keys = keys[0:train_set_size]    \n",
    "        train_data_dict = { key:value for key,value in data_dict.items() if key in train_keys }\n",
    "        test_keys = keys[train_set_size+1:len(keys)]    \n",
    "        test_data_dict = { key:value for key,value in data_dict.items() if key in test_keys }\n",
    "\n",
    "        # run the whole training pipeline\n",
    "        vocab_words, vocab_tags = buildVocab(train_data_dict);\n",
    "        wordcounts = buildWordcountMatrix(train_data_dict, vocab_words, vocab_tags)\n",
    "        tfidf_matrix = computeTFIDF(wordcounts)\n",
    "\n",
    "        # classify all chunks in the test set and compute the error for each\n",
    "        for chunk, tags in test_data_dict.items():\n",
    "            if (baseline):\n",
    "                predictions = classifyChunkRandomGuess(vocab_tags, return_max=return_max)\n",
    "                break\n",
    "            else:\n",
    "                predictions = classifyChunk(chunk, tfidf_matrix, vocab_tags, \n",
    "                                            vocab_words, return_max=return_max)\n",
    "\n",
    "            # skip if no predictions made\n",
    "            if predictions == []: continue                \n",
    "            \n",
    "            # find intersection of predicted and true tags\n",
    "            pred_tags = [tag[0] for tag in predictions]  \n",
    "            tags_intersect = set(tags).intersection(pred_tags)\n",
    "            \n",
    "            # if tags were correctly predicted increase correctly classified item count\n",
    "            if tags_intersect != set(): correct += 1\n",
    "                \n",
    "            # print true labels and predictions\n",
    "            if verbose:\n",
    "                print(\"true labels: \" + str(tags))\n",
    "                print(\"predictions: \" + str(predictions))\n",
    "                print(\"intersection: \" + str(tags_intersect))\n",
    "            \n",
    "        correct_total += correct\n",
    "        test_size_total += test_set_size\n",
    "\n",
    "        # print info on current iteration\n",
    "        print(\"[\" + str(i+1) + \"/\" + str(folds) + \"]\" \n",
    "              + \" Correctly predicted: \" + str(correct) + \" of \" + str(test_set_size) \n",
    "              + \" (\" + \"{:.2f}\".format(correct/test_set_size) + \")\")        \n",
    "\n",
    "\n",
    "    print(\"\\n\" + \"In total (\" + str(folds) + \"-fold CV):\")\n",
    "    print(\"Correctly predicted: \" + str(correct_total) + \" of \" + str(test_size_total) \n",
    "          + \" (\" + \"{:.2f}\".format(correct_total/test_size_total) + \")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting 1: Predicting a Maximum of 5 Tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run 10-fold CV with our classifier while allowing our classifier to predict a maximum of 5 tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10] Correctly predicted: 83 of 291 (0.29)\n",
      "[2/10] Correctly predicted: 90 of 291 (0.31)\n",
      "[3/10] Correctly predicted: 89 of 291 (0.31)\n",
      "[4/10] Correctly predicted: 90 of 291 (0.31)\n",
      "[5/10] Correctly predicted: 92 of 291 (0.32)\n",
      "[6/10] Correctly predicted: 92 of 291 (0.32)\n",
      "[7/10] Correctly predicted: 97 of 291 (0.33)\n",
      "[8/10] Correctly predicted: 93 of 291 (0.32)\n",
      "[9/10] Correctly predicted: 89 of 291 (0.31)\n",
      "[10/10] Correctly predicted: 104 of 291 (0.36)\n",
      "\n",
      "In total (10-fold CV):\n",
      "Correctly predicted: 919 of 2910 (0.32)\n"
     ]
    }
   ],
   "source": [
    "crossvalidate(data_dict, tfidf_matrix, vocab_tags, vocab_words, folds=10, return_max=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll run 10-fold CV the random baseline for comparision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10] Correctly predicted: 0 of 291 (0.00)\n",
      "[2/10] Correctly predicted: 0 of 291 (0.00)\n",
      "[3/10] Correctly predicted: 0 of 291 (0.00)\n",
      "[4/10] Correctly predicted: 0 of 291 (0.00)\n",
      "[5/10] Correctly predicted: 0 of 291 (0.00)\n",
      "[6/10] Correctly predicted: 0 of 291 (0.00)\n",
      "[7/10] Correctly predicted: 0 of 291 (0.00)\n",
      "[8/10] Correctly predicted: 0 of 291 (0.00)\n",
      "[9/10] Correctly predicted: 0 of 291 (0.00)\n",
      "[10/10] Correctly predicted: 0 of 291 (0.00)\n",
      "\n",
      "In total (10-fold CV):\n",
      "Correctly predicted: 0 of 2910 (0.00)\n"
     ]
    }
   ],
   "source": [
    "crossvalidate(data_dict, tfidf_matrix, vocab_tags, \n",
    "              vocab_words, folds=10, return_max=5, baseline=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting 2: Predicting a Maximum of 3 Tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll do the same thing using a maximum of 3 allowed tags for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10] Correctly predicted: 65 of 291 (0.22)\n",
      "[2/10] Correctly predicted: 91 of 291 (0.31)\n",
      "[3/10] Correctly predicted: 66 of 291 (0.23)\n",
      "[4/10] Correctly predicted: 69 of 291 (0.24)\n",
      "[5/10] Correctly predicted: 83 of 291 (0.29)\n",
      "[6/10] Correctly predicted: 64 of 291 (0.22)\n",
      "[7/10] Correctly predicted: 62 of 291 (0.21)\n",
      "[8/10] Correctly predicted: 72 of 291 (0.25)\n",
      "[9/10] Correctly predicted: 72 of 291 (0.25)\n",
      "[10/10] Correctly predicted: 64 of 291 (0.22)\n",
      "\n",
      "In total (10-fold CV):\n",
      "Correctly predicted: 708 of 2910 (0.24)\n"
     ]
    }
   ],
   "source": [
    "crossvalidate(data_dict, tfidf_matrix, vocab_tags, vocab_words, folds=10, return_max=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10] Correctly predicted: 0 of 291 (0.00)\n",
      "[2/10] Correctly predicted: 0 of 291 (0.00)\n",
      "[3/10] Correctly predicted: 0 of 291 (0.00)\n",
      "[4/10] Correctly predicted: 0 of 291 (0.00)\n",
      "[5/10] Correctly predicted: 0 of 291 (0.00)\n",
      "[6/10] Correctly predicted: 0 of 291 (0.00)\n",
      "[7/10] Correctly predicted: 0 of 291 (0.00)\n",
      "[8/10] Correctly predicted: 0 of 291 (0.00)\n",
      "[9/10] Correctly predicted: 0 of 291 (0.00)\n",
      "[10/10] Correctly predicted: 0 of 291 (0.00)\n",
      "\n",
      "In total (10-fold CV):\n",
      "Correctly predicted: 0 of 2910 (0.00)\n"
     ]
    }
   ],
   "source": [
    "crossvalidate(data_dict, tfidf_matrix, vocab_tags, \n",
    "              vocab_words, folds=10, return_max=3, baseline=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting 3: Predicting Only the Most Likely Tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And again with only the most likely tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10] Correctly predicted: 41 of 291 (0.14)\n",
      "[2/10] Correctly predicted: 31 of 291 (0.11)\n",
      "[3/10] Correctly predicted: 37 of 291 (0.13)\n",
      "[4/10] Correctly predicted: 41 of 291 (0.14)\n",
      "[5/10] Correctly predicted: 42 of 291 (0.14)\n",
      "[6/10] Correctly predicted: 35 of 291 (0.12)\n",
      "[7/10] Correctly predicted: 38 of 291 (0.13)\n",
      "[8/10] Correctly predicted: 33 of 291 (0.11)\n",
      "[9/10] Correctly predicted: 36 of 291 (0.12)\n",
      "[10/10] Correctly predicted: 36 of 291 (0.12)\n",
      "\n",
      "In total (10-fold CV):\n",
      "Correctly predicted: 370 of 2910 (0.13)\n"
     ]
    }
   ],
   "source": [
    "crossvalidate(data_dict, tfidf_matrix, vocab_tags, vocab_words, folds=10, return_max=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10] Correctly predicted: 0 of 291 (0.00)\n",
      "[2/10] Correctly predicted: 0 of 291 (0.00)\n",
      "[3/10] Correctly predicted: 0 of 291 (0.00)\n",
      "[4/10] Correctly predicted: 0 of 291 (0.00)\n",
      "[5/10] Correctly predicted: 0 of 291 (0.00)\n",
      "[6/10] Correctly predicted: 0 of 291 (0.00)\n",
      "[7/10] Correctly predicted: 0 of 291 (0.00)\n",
      "[8/10] Correctly predicted: 0 of 291 (0.00)\n",
      "[9/10] Correctly predicted: 0 of 291 (0.00)\n",
      "[10/10] Correctly predicted: 0 of 291 (0.00)\n",
      "\n",
      "In total (10-fold CV):\n",
      "Correctly predicted: 0 of 2910 (0.00)\n"
     ]
    }
   ],
   "source": [
    "crossvalidate(data_dict, tfidf_matrix, vocab_tags, \n",
    "              vocab_words, folds=10, return_max=1, baseline=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first learning was that people don't follow instructions and e.g. empty sections were tagged, leading to zero division errors due to the word frequencies being zero. Also tags were partially done in Finnish and were sometimes not seperated by comma but rather marked with the hash sign (e.g. #finlandjob). Some of these mistakes were directly corrected in the preprocessing while others weren't yet (seperating tags is not done yet)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the intersection of tags is a proper metric to measure performance of a multiclass predictor the results are surprisingly good using the rather simple TF.IDF method. While the random baseline does not get a single guess right, the classifier predicts matching classes in around on third of the cases for the first setting with 5 output classes. Even when allowing only 1 output class it gets a match in 12% of the cases. \n",
    "\n",
    "This result is especially good considering that tags are not processed in any way, meaning that they the predicted tags have to match literally (\"languages\" does not match \"language\"). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion and Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the experiment gave good results with the chosen metrix, several issues or potential improvements can be pointed that could be further developed.\n",
    "\n",
    "## Improved Preprocessing for Reducing Effects of Human Error\n",
    "In not so fancy words this means that some tags were not seperated by comma as mentioned in the result section and instead tagged by a hash character. This and other obvious and common mistakes should be resolved in the preprocessing step.\n",
    "\n",
    "## Metrics for multiclass classification\n",
    "The metric for multiclass classification is somewhat questionable and should be defined better. One possible way to design the metric could be to output confidence results by taking the proportion of tags that match. On the other hand there is also a lot of noise in the data and it is questionable if the true labels given can be seen as a real ground truth to be learned as closely as possible or rather a noisy dataset to generalize from. \n",
    "\n",
    "## Removing Redundancy \n",
    "Many words and tags have redundant representations in this application context (\"language\" and \"languages\" refer to the same concept). This makes it harder for the classification to predict the correct results and could be improved with the following approaches: \n",
    "\n",
    "1. Thresholding frequencies: \n",
    "First words and tags that only appear $n$ times (e.g. only once) could be removed as they don't carry much information.\n",
    "\n",
    "2. Lemmatization: \n",
    "So far there was no real lemmatizing of words and tags was done (apart from lowercasing them). Doing this would greatly help to reduce the amount of redundancy in the mapping and almost certainly improve the results significantly.  This could also involve resolving synomyms using e.g. WordNet.\n",
    "\n",
    "3. Using Relations in the Vector Space:\n",
    "As shown in Appendix C the TF.IDF vector space can be used to identify very similar tags. This could help to even further reduce redundant tags that are not captured by any lemmatization (e.g. synomnyms like \"skills\" and \"requirements\"). This needs some more experimentation though as the distance is not the only relation between the tag vectors (think of subsets of words describing tag that belongs to a sub-hierarchy of another)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An interesting side-effect was that since some people did not read the instructions and tagged in Finnish the mapping effectively acts as a translation (see Appendix C). This could be further looked into."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Leskovec J, Rajaraman A, Ullman JD (2014)  Mining of massive datasets. Cambridge University Press. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A: Best of Tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for fun here's a list with the most interesting or surprising tags that participants used:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 90% bullshit (I wonder how that is measured)\n",
    "2. bullshit\n",
    "3. selling the job\n",
    "4. empy (yes \"empy\", there's also empty which doesn't make sense either since empty sections should be ignored)\n",
    "5. \"#kemira #finlandjob\", (tags not seperated by comma and use the #)\n",
    "6. useless info\n",
    "7. crap\n",
    "8. kuvaus (etc. somebody tagged in Finnish)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: People don't follow (or read) instructions. Nevertheless I am of course extremely thankful for all the participants :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B: Finding the Most Likely Words for Each Tag and vice Versa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute a distance matrix between the tag vectors (classes) to find very similar classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getMostLikelyWords(tag, tfidf_matrix, vocab_words, vocab_tags):\n",
    "    tag_id = tag2id(tag, vocab_tags)\n",
    "    nonzeros = tfidf_matrix[:,tag_id].nonzero()[0]\n",
    "    return_tags = {}\n",
    "    for word_id in nonzeros:\n",
    "        return_tags[id2word(word_id, vocab_words)] = tfidf_matrix[word_id, tag_id]\n",
    "    return sorted(return_tags.items(), key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getMostLikelyTags(word, tfidf_matrix, vocab_words, vocab_tags):\n",
    "    word_id = word2id(word, vocab_words)\n",
    "    nonzeros = tfidf_matrix[word_id,:].nonzero()[1]\n",
    "    return_tags = {}\n",
    "    for tag_id in nonzeros:\n",
    "        return_tags[id2tag(tag_id, vocab_tags)] = tfidf_matrix[word_id, tag_id]\n",
    "    return sorted(return_tags.items(), key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('language', 3.4642810182986929),\n",
       " ('language skill', 3.4642810182986929),\n",
       " ('languages', 3.4642810182986929),\n",
       " ('language requirements', 3.4642810182986929),\n",
       " ('skill exception', 3.4642810182986929),\n",
       " ('actual requirements', 3.4642810182986929),\n",
       " ('job requirements', 2.3095206788657952),\n",
       " ('important requirements', 1.7321405091493465),\n",
       " ('level', 1.7321405091493465),\n",
       " ('common requirements', 1.7321405091493465),\n",
       " ('tech', 1.7321405091493465),\n",
       " ('innovation', 1.7321405091493465),\n",
       " ('contact agent', 1.7321405091493465),\n",
       " ('habits', 1.7321405091493465),\n",
       " ('language skills', 1.3857124073194773),\n",
       " ('requirements', 1.3857124073194773),\n",
       " ('expectations', 1.1547603394328976),\n",
       " ('about', 1.1547603394328976),\n",
       " ('language requirement', 0.86607025457467324),\n",
       " ('skill', 0.76984022628859838),\n",
       " ('summary', 0.74234593249257697),\n",
       " ('skills', 0.70860293556109633),\n",
       " ('role', 0.69285620365973866),\n",
       " ('requirement', 0.57738016971644879),\n",
       " ('responsibilities', 0.53296631050749121),\n",
       " ('experience', 0.33525300177084122),\n",
       " ('tasks', 0.28869008485822439)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getMostLikelyTags(\"finnish\", tfidf_matrix, vocab_words, vocab_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('norwegian', 7.2191685204621612),\n",
       " ('speaking', 7.2191685204621612),\n",
       " ('writing', 6.6342060197410051),\n",
       " ('fluency', 4.8972404255747994),\n",
       " ('least', 4.7597369018248639),\n",
       " ('fluent', 3.8268510976834014),\n",
       " ('finnish', 3.4642810182986929),\n",
       " ('english', 2.971241007018576)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getMostLikelyWords(\"language\", tfidf_matrix, vocab_words, vocab_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C: Computing a Distance Matrix Between Tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the TF.IDF matrix to compute the most likely words given a tag and vice versa: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jaccard_kernel = sklearn.metrics.pairwise_distances(tfidf_matrix.transpose().A, \n",
    "                                                    metric=\"jaccard\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to find similar tags for each tag. (Note: the distance is given in brackets, not the similarity):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wished skills: start of skills list(0.88), heading(0.88), department(0.88), useless info(0.88), ask(0.88), search(0.88), soft skills(0.88), tools(0.88), \n",
      "position type: extent(0.75), employment type(0.75), job form(0.75), \n",
      "extent: position type(0.75), employment type(0.75), job form(0.75), \n",
      "header: section title(0.53), \n",
      "details: about job agent(0.84), \n",
      "offering: employer's information(0.60), company(0.60), \n",
      "title: position level(0.34), task titles(0.34), team title(0.34), \n",
      "contact ddetails: contact details(0.83), job desription(0.83), \n",
      "job benefits: travel requirements(0.73), oppoturnity(0.73), \n",
      "additional job opportunities: application deadline(0.85), \n",
      "dealine: about job agent(0.86), \n",
      "previous experience: background(0.83), job specific tag(0.83), \n",
      "product line: company details(0.00), \n",
      "company details: product line(0.00), \n",
      "#kemira #finlandjob: department(0.83), what(0.83), where(0.83), who(0.83), need(0.83), ask(0.83), search(0.83), work title(0.83), \n",
      "about: unit description(0.77), \n",
      "unit description: about(0.00), \n",
      "location of job: job classification(0.86), \n",
      "actual requirements: languages(0.88), \n",
      "application information: application deadline(0.78), website contact info(0.78), application practicalities(0.78), \n",
      "habits: important requirements(0.88), technical skills(0.88), \n",
      "important requirements: habits(0.88), technical skills(0.88), \n",
      "start of skills list: wished skills(0.00), \n",
      "employer listing: further information(0.85), \n",
      "further information: employer listing(0.00), \n",
      "requirements title: additional information(0.88), prerequisite studies(0.88), application procedure(0.88), start of list(0.88), deadline for application(0.88), \n",
      "communication skill: 90% bullshit(0.60), communication(0.60), \n",
      "employer's information: offering(0.56), company(0.56), \n",
      "company: offering(0.00), employer's information(0.00), \n",
      "about job agent: details(0.00), dealine(0.00), \n",
      "crap: job contact information(0.75), job number(0.75), number(0.75), travel requirements(0.75), tracking(0.75), \n",
      "heading: wished skills(0.75), department(0.75), useless info(0.75), ask(0.75), search(0.75), \n",
      "language skill: bonus skills(0.79), languages(0.79), skill exception(0.79), language(0.79), \n",
      "attachments: application procedure(0.86), deadline for application(0.86), how to apply?(0.86), application practicalities(0.86), \n",
      "bonus skills: language skill(0.00), \n",
      "contact details: contact ddetails(0.83), job desription(0.83), \n",
      "competence: communication skills(0.75), 90% bullshit(0.75), communication(0.75), \n",
      "closing date: opening date(0.83), dates(0.83), \n",
      "additional information: requirements title(0.67), prerequisite studies(0.67), further info(0.67), more info(0.67), start of list(0.67), contact practicalities(0.67), \n",
      "prerequisite studies: requirements title(0.83), additional information(0.83), start of list(0.83), \n",
      "responsibilities heading: checklist for job description(0.67), \n",
      "department: wished skills(0.81), #kemira #finlandjob(0.81), heading(0.81), what(0.81), where(0.81), who(0.81), useless info(0.81), ask(0.81), search(0.81), work title(0.81), \n",
      "contact person title: employer description(0.83), \n",
      "innovation: tech(0.00), \n",
      "tech: innovation(0.00), \n",
      "time commitment: time(0.80), employment type(0.80), info(0.80), job classification(0.80), \n",
      "time: time commitment(0.80), employment type(0.80), info(0.80), job classification(0.80), \n",
      "employment type: position type(0.62), extent(0.62), time commitment(0.62), time(0.62), info(0.62), job form(0.62), \n",
      "info: time commitment(0.80), time(0.80), employment type(0.80), job classification(0.80), \n",
      "what: #kemira #finlandjob(0.82), department(0.82), where(0.82), who(0.82), need(0.82), ask(0.82), search(0.82), work title(0.82), \n",
      "where: #kemira #finlandjob(0.82), department(0.82), what(0.82), who(0.82), need(0.82), ask(0.82), search(0.82), work title(0.82), \n",
      "who: #kemira #finlandjob(0.82), department(0.82), what(0.82), where(0.82), need(0.82), ask(0.82), search(0.82), work title(0.82), \n",
      "conditions: information(0.58), \n",
      "application deadline: additional job opportunities(0.87), application information(0.87), website contact info(0.87), encouragement(0.87), invitation(0.87), \n",
      "communication skills: competence(0.71), 90% bullshit(0.71), communication(0.71), \n",
      "further info: additional information(0.00), more info(0.00), contact practicalities(0.00), \n",
      "job contact information: crap(0.75), job number(0.75), number(0.75), travel requirements(0.75), tracking(0.75), \n",
      "job number: crap(0.75), job contact information(0.75), number(0.75), travel requirements(0.75), tracking(0.75), \n",
      "job id: website of employer(0.50), application procedure(0.50), deadline for application(0.50), \n",
      "website of employer: job id(0.50), application procedure(0.50), deadline for application(0.50), \n",
      "employer info: job related field(0.00), \n",
      "job related field: employer info(0.00), \n",
      "more info: additional information(0.80), further info(0.80), contact practicalities(0.80), \n",
      "work experience: background(0.83), job specific tag(0.83), \n",
      "opening date: closing date(0.83), dates(0.83), \n",
      "need: #kemira #finlandjob(0.83), what(0.83), where(0.83), who(0.83), job duration(0.83), work title(0.83), \n",
      "section title: header(0.00), \n",
      "background: previous experience(0.88), work experience(0.88), job specific tag(0.88), programming skills(0.88), signal for interview question(0.88), tools(0.88), \n",
      "website contact info: application information(0.78), application deadline(0.78), application practicalities(0.78), \n",
      "encouragement: application deadline(0.88), invitation(0.88), job classification(0.88), application practicalities(0.88), \n",
      "invitation: application deadline(0.00), encouragement(0.00), \n",
      "personal: company culture(0.00), \n",
      "company culture: personal(0.00), \n",
      "non-discrimination: equal opportunity(0.00), company policy(0.00), \n",
      "equal opportunity: non-discrimination(0.00), company policy(0.00), \n",
      "company policy: non-discrimination(0.00), equal opportunity(0.00), \n",
      "compensation: marketing of the company(0.88), treats(0.88), \n",
      "marketing of the company: compensation(0.88), treats(0.88), \n",
      "languages: actual requirements(0.70), language skill(0.70), skill exception(0.70), language(0.70), \n",
      "number: crap(0.75), job contact information(0.75), job number(0.75), travel requirements(0.75), tracking(0.75), \n",
      "langugage skills: technical skills(0.07), \n",
      "useless info: wished skills(0.50), heading(0.50), department(0.50), ask(0.50), search(0.50), \n",
      "time frame: job(0.88), #clinicalcontractcoordinator #bayerrecruiting(0.88), \n",
      "job: time frame(0.00), \n",
      "job type: kuvaus(0.67), \n",
      "analysis: mobile development(0.46), \n",
      "job duration: need(0.86), practicalities(0.86), \n",
      "practicalities: job duration(0.84), introduction to organization(0.84), \n",
      "java: programming skills(0.80), \n",
      "90% bullshit: communication skill(0.86), competence(0.86), communication skills(0.86), communication(0.86), performance(0.86), \n",
      "#clinicalcontractcoordinator #bayerrecruiting: time frame(0.00), \n",
      "job specific tag: previous experience(0.86), work experience(0.86), background(0.86), programming skills(0.86), tools(0.86), \n",
      "technical skills: habits(0.00), important requirements(0.00), langugage skills(0.00), \n",
      "skill exception: language skill(0.88), languages(0.88), language(0.88), job desription(0.88), \n",
      "introduction to organization: practicalities(0.00), \n",
      "language: language skill(0.00), languages(0.00), skill exception(0.00), \n",
      "job form: position type(0.00), extent(0.00), employment type(0.00), \n",
      "applicant's personality: applicant's abilities(0.00), \n",
      "applicant's abilities: applicant's personality(0.00), \n",
      "employer description: contact person title(0.00), \n",
      "job classification: location of job(0.00), time commitment(0.00), time(0.00), info(0.00), encouragement(0.00), \n",
      "programming skills: background(0.00), java(0.00), job specific tag(0.00), \n",
      "ask: wished skills(0.86), #kemira #finlandjob(0.86), heading(0.86), department(0.86), what(0.86), where(0.86), who(0.86), useless info(0.86), search(0.86), work title(0.86), \n",
      "search: wished skills(0.86), #kemira #finlandjob(0.86), heading(0.86), department(0.86), what(0.86), where(0.86), who(0.86), useless info(0.86), ask(0.86), work title(0.86), \n",
      "checklist for job description: responsibilities heading(0.75), start of list(0.75), \n",
      "signal for interview question: background(0.88), job desription(0.88), \n",
      "travel requirements: job benefits(0.83), crap(0.83), job contact information(0.83), job number(0.83), number(0.83), tracking(0.83), \n",
      "mobile development: analysis(0.00), \n",
      "communication: communication skill(0.88), competence(0.88), communication skills(0.88), 90% bullshit(0.88), soft skills(0.88), performance(0.88), istqb(0.88), \n",
      "application procedure: requirements title(0.88), attachments(0.88), job id(0.88), website of employer(0.88), deadline for application(0.88), tracking(0.88), \n",
      "position level: title(0.00), task titles(0.00), team title(0.00), \n",
      "task titles: title(0.00), position level(0.00), team title(0.00), \n",
      "team title: title(0.00), position level(0.00), task titles(0.00), \n",
      "soft skills: wished skills(0.00), communication(0.00), \n",
      "start of list: requirements title(0.00), additional information(0.00), prerequisite studies(0.00), checklist for job description(0.00), \n",
      "treats: compensation(0.00), marketing of the company(0.00), \n",
      "oppoturnity: job benefits(0.00), \n",
      "tools: wished skills(0.83), background(0.83), job specific tag(0.83), design(0.83), \n",
      "performance: 90% bullshit(0.00), communication(0.00), \n",
      "deadline for application: requirements title(0.88), attachments(0.88), job id(0.88), website of employer(0.88), application procedure(0.88), tracking(0.88), \n",
      "dates: closing date(0.00), opening date(0.00), \n",
      "transition: expected values(0.88), job position(0.88), application practicalities(0.88), \n",
      "design: tools(0.00), \n",
      "istqb: communication(0.00), \n",
      "traveling requirements: employee qualities(0.00), \n",
      "employee qualities: traveling requirements(0.00), \n",
      "job desription: contact ddetails(0.00), contact details(0.00), skill exception(0.00), signal for interview question(0.00), \n",
      "field of operations: unit of company(0.00), \n",
      "unit of company: field of operations(0.00), \n",
      "tracking: crap(0.00), job contact information(0.00), job number(0.00), number(0.00), travel requirements(0.00), application procedure(0.00), deadline for application(0.00), \n",
      "expected values: transition(0.00), \n",
      "how to apply?: attachments(0.00), \n",
      "contact practicalities: additional information(0.00), further info(0.00), more info(0.00), \n",
      "job position: transition(0.86), introductory phrase(0.86), \n",
      "kuvaus: job type(0.00), \n",
      "introductory phrase: job position(0.00), \n",
      "project management: bugtracking tool(0.00), \n",
      "bugtracking tool: project management(0.00), \n",
      "information: conditions(0.00), \n",
      "location data: ccompany details(0.00), \n",
      "ccompany details: location data(0.00), \n",
      "application practicalities: application information(0.00), attachments(0.00), website contact info(0.00), encouragement(0.00), transition(0.00), \n",
      "work title: #kemira #finlandjob(0.00), department(0.00), what(0.00), where(0.00), who(0.00), need(0.00), ask(0.00), search(0.00), \n"
     ]
    }
   ],
   "source": [
    "similarTags = []\n",
    "for i, row in enumerate(jaccard_kernel):    \n",
    "    similarTags.append([])\n",
    "    for (j, value) in [(tag_id, value) \n",
    "                       for tag_id, value in enumerate(jaccard_kernel[i,:]) if value < 0.88]:\n",
    "        if (j == i): continue\n",
    "        similarTags[i].append(j)\n",
    "    \n",
    "    if similarTags[i] != []:\n",
    "        sys.stdout.write(id2tag(i, vocab_tags) + \": \")\n",
    "        for tag in similarTags[i]:\n",
    "            sys.stdout.write(id2tag(tag, vocab_tags) + \"(\" + \"{:.2f}\".format(value) + \"), \")\n",
    "        sys.stdout.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that in many cases semantically similar concepts were mapped close to each other in the vector space which is a very good sign. An side-effect is that also some Finnish tags (while they should not exists) are effectively translated this way, e.g. \"kuvaus\" meaning \"description\" maps to \"job type\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
